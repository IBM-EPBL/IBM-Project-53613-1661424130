{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN5rg1hBdQhu1qs58O5Nlie"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#FOR BODY DAMAGE\n","\n","#IMAGE PRE PROCESSING\n","\n","#1. Import The ImageDataGenerator Library\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","#2. Configure ImageDataGenerator Class\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.1,\n","                                   zoom_range = 0.1,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","#3. Apply ImageDataGenerator Functionality To Trainset And Testset\n","\n","training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training',\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 10,\n","                                                 class_mode = 'categorical')\n","test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/validation',\n","                                            target_size = (224, 224),\n","                                            batch_size = 10,\n","                                            class_mode = 'categorical')\n","\n","#MODEL BUILDING\n","\n","#1. Importing The Model Building Libraries\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg19 import VGG19\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n","from tensorflow.keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","#2. Loading The Model\n","\n","IMAGE_SIZE = [224, 224]\n","\n","train_path = '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training'\n","valid_path = '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/validation'\n","vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n","#3. Adding Flatten Layer\n","\n","for layer in vgg16.layers:\n","    layer.trainable = False\n","folders = glob('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training/*')\n","folders\n","['/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training/02-side',\n"," '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training/00-front',\n"," '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training/01-rear']\n","x = Flatten()(vgg16.output)\n","len(folders)\n","3\n","#4. Adding Output Layer\n","\n","prediction = Dense(len(folders), activation='softmax')(x)\n","#5. Creating A Model Object\n","\n","model = Model(inputs=vgg16.input, outputs=prediction)\n","model.summary()\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 3)                 75267     \n","                                                                 \n","=================================================================\n","Total params: 14,789,955\n","Trainable params: 75,267\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","#6. Configure The Learning Process\n","\n","model.compile(\n","  loss='categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")\n","#7. Train The Model\n","\n","r = model.fit_generator(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=25,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  \n","Epoch 1/25\n","98/98 [==============================] - 606s 6s/step - loss: 1.2827 - accuracy: 0.5649 - val_loss: 0.8292 - val_accuracy: 0.7076\n","Epoch 2/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.6301 - accuracy: 0.7467 - val_loss: 1.2482 - val_accuracy: 0.5965\n","Epoch 3/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.5073 - accuracy: 0.8039 - val_loss: 0.8174 - val_accuracy: 0.7193\n","Epoch 4/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.3564 - accuracy: 0.8621 - val_loss: 0.9245 - val_accuracy: 0.6608\n","Epoch 5/25\n","98/98 [==============================] - 599s 6s/step - loss: 0.2951 - accuracy: 0.8917 - val_loss: 1.9934 - val_accuracy: 0.5906\n","Epoch 6/25\n","98/98 [==============================] - 638s 7s/step - loss: 0.2557 - accuracy: 0.9152 - val_loss: 0.9176 - val_accuracy: 0.6842\n","Epoch 7/25\n","98/98 [==============================] - 607s 6s/step - loss: 0.2083 - accuracy: 0.9367 - val_loss: 0.9594 - val_accuracy: 0.7018\n","Epoch 8/25\n","98/98 [==============================] - 600s 6s/step - loss: 0.2184 - accuracy: 0.9122 - val_loss: 1.0329 - val_accuracy: 0.6784\n","Epoch 9/25\n","98/98 [==============================] - 602s 6s/step - loss: 0.1320 - accuracy: 0.9581 - val_loss: 1.0539 - val_accuracy: 0.7135\n","Epoch 10/25\n","98/98 [==============================] - 599s 6s/step - loss: 0.1131 - accuracy: 0.9622 - val_loss: 1.2113 - val_accuracy: 0.6842\n","Epoch 11/25\n","98/98 [==============================] - 597s 6s/step - loss: 0.1001 - accuracy: 0.9745 - val_loss: 0.9917 - val_accuracy: 0.7018\n","Epoch 12/25\n","98/98 [==============================] - 598s 6s/step - loss: 0.0954 - accuracy: 0.9745 - val_loss: 1.0601 - val_accuracy: 0.7018\n","Epoch 13/25\n","98/98 [==============================] - 594s 6s/step - loss: 0.0695 - accuracy: 0.9816 - val_loss: 1.3700 - val_accuracy: 0.6433\n","Epoch 14/25\n","98/98 [==============================] - 599s 6s/step - loss: 0.1414 - accuracy: 0.9653 - val_loss: 1.1607 - val_accuracy: 0.6667\n","Epoch 15/25\n","98/98 [==============================] - 600s 6s/step - loss: 0.0905 - accuracy: 0.9796 - val_loss: 1.4014 - val_accuracy: 0.6667\n","Epoch 16/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.0797 - accuracy: 0.9775 - val_loss: 1.6741 - val_accuracy: 0.6491\n","Epoch 17/25\n","98/98 [==============================] - 602s 6s/step - loss: 0.1042 - accuracy: 0.9745 - val_loss: 1.2824 - val_accuracy: 0.6959\n","Epoch 18/25\n","98/98 [==============================] - 600s 6s/step - loss: 0.0831 - accuracy: 0.9785 - val_loss: 1.1667 - val_accuracy: 0.6901\n","Epoch 19/25\n","98/98 [==============================] - 603s 6s/step - loss: 0.0826 - accuracy: 0.9704 - val_loss: 1.3747 - val_accuracy: 0.6374\n","Epoch 20/25\n","98/98 [==============================] - 600s 6s/step - loss: 0.0536 - accuracy: 0.9837 - val_loss: 1.2074 - val_accuracy: 0.6550\n","Epoch 21/25\n","98/98 [==============================] - 597s 6s/step - loss: 0.0716 - accuracy: 0.9796 - val_loss: 1.5491 - val_accuracy: 0.6725\n","Epoch 22/25\n","98/98 [==============================] - 599s 6s/step - loss: 0.0457 - accuracy: 0.9918 - val_loss: 1.2930 - val_accuracy: 0.7135\n","Epoch 23/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.0526 - accuracy: 0.9928 - val_loss: 1.2576 - val_accuracy: 0.6959\n","Epoch 24/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.0421 - accuracy: 0.9908 - val_loss: 1.3347 - val_accuracy: 0.7193\n","Epoch 25/25\n","98/98 [==============================] - 597s 6s/step - loss: 0.0597 - accuracy: 0.9826 - val_loss: 1.4728 - val_accuracy: 0.6725\n","#8. Save The Model\n","\n","from tensorflow.keras.models import load_model\n","\n","model.save('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Model/body.h5')\n","#9. Test The Model\n","\n","from tensorflow.keras.models import load_model\n","import cv2\n","from skimage.transform import resize\n","model = load_model('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Model/body.h5')\n","def detect(frame):\n","  img = cv2.resize(frame,(224,224))\n","  img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","\n","  if(np.max(img)>1):\n","    img = img/255.0\n","  img = np.array([img])\n","  prediction = model.predict(img)\n","  label = [\"front\",\"rear\",\"side\"]\n","  preds = label[np.argmax(prediction)]\n","  return preds\n","import numpy as np\n","data = \"/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/body/training/00-front/0005.JPEG\"\n","image = cv2.imread(data)\n","print(detect(image))\n","\n","#FOR LEVEL DAMAGE\n","\n","#IMAGE PRE PROCESSING\n","\n","#1. Import The ImageDataGenerator Library\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","#2. Configure ImageDataGenerator Class\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.1,\n","                                   zoom_range = 0.1,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","#3. Apply ImageDataGenerator Functionality To Trainset And Testset\n","\n","training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/training',\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 10,\n","                                                 class_mode = 'categorical')\n","test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/validation',\n","                                            target_size = (224, 224),\n","                                            batch_size = 10,\n","                                            class_mode = 'categorical')\n","Found 979 images belonging to 3 classes.\n","Found 171 images belonging to 3 classes.\n","#MODEL BUILDING\n","\n","#1. Importing The Model Building Libraries\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg19 import VGG19\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n","from tensorflow.keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","#2. Loading The Model\n","\n","IMAGE_SIZE = [224, 224]\n","\n","train_path = '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/training'\n","valid_path = '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/validation'\n","vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n","\n","#3. Adding Flatten Layer\n","\n","for layer in vgg16.layers:\n","    layer.trainable = False\n","folders = glob('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/training/*')\n","folders\n","['/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/training/02-moderate',\n"," '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/training/03-severe',\n"," '/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/training/01-minor']\n","x = Flatten()(vgg16.output)\n","len(folders)\n","3\n","#4. Adding Output Layer\n","\n","prediction = Dense(len(folders), activation='softmax')(x)\n","#5. Creating A Model Object\n","\n","model = Model(inputs=vgg16.input, outputs=prediction)\n","model.summary()\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 3)                 75267     \n","                                                                 \n","=================================================================\n","Total params: 14,789,955\n","Trainable params: 75,267\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","#6. Configure The Learning Process\n","\n","model.compile(\n","  loss='categorical_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")\n","7. Train The Model\n","\n","r = model.fit_generator(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=25,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  \n","Epoch 1/25\n","98/98 [==============================] - 615s 6s/step - loss: 1.2465 - accuracy: 0.5516 - val_loss: 1.0659 - val_accuracy: 0.5731\n","Epoch 2/25\n","98/98 [==============================] - 604s 6s/step - loss: 0.6654 - accuracy: 0.7549 - val_loss: 1.0368 - val_accuracy: 0.6316\n","Epoch 3/25\n","98/98 [==============================] - 604s 6s/step - loss: 0.5950 - accuracy: 0.7630 - val_loss: 1.1309 - val_accuracy: 0.6257\n","Epoch 4/25\n","98/98 [==============================] - 601s 6s/step - loss: 0.4964 - accuracy: 0.8069 - val_loss: 1.1262 - val_accuracy: 0.6082\n","Epoch 5/25\n","98/98 [==============================] - 603s 6s/step - loss: 0.3559 - accuracy: 0.8672 - val_loss: 1.1408 - val_accuracy: 0.6316\n","Epoch 6/25\n","98/98 [==============================] - 604s 6s/step - loss: 0.2425 - accuracy: 0.9152 - val_loss: 1.1566 - val_accuracy: 0.5789\n","Epoch 7/25\n","98/98 [==============================] - 604s 6s/step - loss: 0.1964 - accuracy: 0.9367 - val_loss: 1.1200 - val_accuracy: 0.6199\n","Epoch 8/25\n","98/98 [==============================] - 598s 6s/step - loss: 0.2119 - accuracy: 0.9203 - val_loss: 1.1181 - val_accuracy: 0.6316\n","Epoch 9/25\n","98/98 [==============================] - 597s 6s/step - loss: 0.1111 - accuracy: 0.9622 - val_loss: 1.3554 - val_accuracy: 0.5614\n","Epoch 10/25\n","98/98 [==============================] - 595s 6s/step - loss: 0.1394 - accuracy: 0.9438 - val_loss: 1.2256 - val_accuracy: 0.6082\n","Epoch 11/25\n","98/98 [==============================] - 598s 6s/step - loss: 0.1167 - accuracy: 0.9602 - val_loss: 1.3020 - val_accuracy: 0.6374\n","Epoch 12/25\n","98/98 [==============================] - 598s 6s/step - loss: 0.0823 - accuracy: 0.9755 - val_loss: 1.3000 - val_accuracy: 0.6550\n","Epoch 13/25\n","98/98 [==============================] - 602s 6s/step - loss: 0.1062 - accuracy: 0.9632 - val_loss: 1.2962 - val_accuracy: 0.6433\n","Epoch 14/25\n","98/98 [==============================] - 599s 6s/step - loss: 0.0717 - accuracy: 0.9775 - val_loss: 1.3089 - val_accuracy: 0.6491\n","Epoch 15/25\n","98/98 [==============================] - 598s 6s/step - loss: 0.0692 - accuracy: 0.9826 - val_loss: 1.2885 - val_accuracy: 0.6023\n","Epoch 16/25\n","98/98 [==============================] - 595s 6s/step - loss: 0.0449 - accuracy: 0.9898 - val_loss: 1.7932 - val_accuracy: 0.5673\n","Epoch 17/25\n","98/98 [==============================] - 609s 6s/step - loss: 0.0522 - accuracy: 0.9867 - val_loss: 1.2697 - val_accuracy: 0.6433\n","Epoch 18/25\n","98/98 [==============================] - 607s 6s/step - loss: 0.0386 - accuracy: 0.9969 - val_loss: 1.5100 - val_accuracy: 0.6023\n","Epoch 19/25\n","98/98 [==============================] - 595s 6s/step - loss: 0.0381 - accuracy: 0.9939 - val_loss: 1.2199 - val_accuracy: 0.6784\n","Epoch 20/25\n","98/98 [==============================] - 596s 6s/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.2907 - val_accuracy: 0.6433\n","Epoch 21/25\n","98/98 [==============================] - 597s 6s/step - loss: 0.0394 - accuracy: 0.9928 - val_loss: 1.2678 - val_accuracy: 0.6491\n","Epoch 22/25\n","98/98 [==============================] - 595s 6s/step - loss: 0.0377 - accuracy: 0.9908 - val_loss: 1.4709 - val_accuracy: 0.6316\n","Epoch 23/25\n","98/98 [==============================] - 595s 6s/step - loss: 0.0387 - accuracy: 0.9918 - val_loss: 1.3320 - val_accuracy: 0.6257\n","Epoch 24/25\n","98/98 [==============================] - 596s 6s/step - loss: 0.0279 - accuracy: 0.9949 - val_loss: 1.6355 - val_accuracy: 0.6433\n","Epoch 25/25\n","98/98 [==============================] - 603s 6s/step - loss: 0.0271 - accuracy: 0.9939 - val_loss: 1.3182 - val_accuracy: 0.6608\n","#8. Save The Model\n","\n","from tensorflow.keras.models import load_model\n","\n","model.save('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Model/level.h5')\n","#9. Test The Model\n","\n","from tensorflow.keras.models import load_model\n","import cv2\n","from skimage.transform import resize\n","model = load_model('/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Model/level.h5')\n","def detect(frame):\n","  img = cv2.resize(frame,(224,224))\n","  img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","\n","  if(np.max(img)>1):\n","    img = img/255.0\n","  img = np.array([img])\n","  prediction = model.predict(img)\n","  label = [\"minor\",\"moderate\",\"severe\"]\n","  preds = label[np.argmax(prediction)]\n","  return preds\n","import numpy as np\n","data = \"/content/drive/MyDrive/Intelligent Vehicle Damage Assessment & Cost Estimator For Insurance Companies/Dataset/level/validation/01-minor/0010.JPEG\"\n","image = cv2.imread(data)\n","print(detect(image))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"0Nen5AoWu07s","executionInfo":{"status":"error","timestamp":1668372830993,"user_tz":480,"elapsed":439,"user":{"displayName":"Aisha H","userId":"17910085311473628931"}},"outputId":"2fd7e23e-623c-44ce-ac2a-0d9c6bc91995"},"execution_count":2,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ad564430d6d1>\"\u001b[0;36m, line \u001b[0;32m69\u001b[0m\n\u001b[0;31m    Layer (type)                Output Shape              Param #\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]}]}